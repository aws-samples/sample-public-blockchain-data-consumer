AWSTemplateFormatVersion: '2010-09-09'
Description: >
  AWS Public Blockchain Data Consumer Architecture with Automated Schema Discovery.
  This template extends the base architecture with Glue Crawlers that automatically
  discover and catalog new blockchain data schemas.

Parameters:
  S3Bucket:
    Description: The S3 bucket with the AWS Public Blockchain Data
    Type: String
    Default: "aws-public-blockchain"
  
  SchemaVersion:
    Type: String
    Default: "v1.0"
    Description: Default schema version for BTC/ETH
  
  SchemaVersionTON:
    Type: String
    Default: "v1.1"
    Description: Schema version for TON blockchain
  
  DiscoverySchedule:
    Type: String
    Default: "cron(0 2 ? * SUN *)"
    Description: Schedule for blockchain discovery (default weekly on Sunday at 2 AM UTC)
  
  DefaultCrawlerSchedule:
    Type: String
    Default: "daily"
    AllowedValues:
      - "hourly"
      - "daily"
      - "weekly"
      - "disabled"
    Description: Default schedule for per-blockchain crawlers (can be changed per-chain via CLI)
  
  EnableAutoCrawling:
    Type: String
    Default: "true"
    AllowedValues:
      - "true"
      - "false"
    Description: Enable automatic crawler scheduling

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Data Source Configuration"
        Parameters:
          - S3Bucket
          - SchemaVersion
          - SchemaVersionTON
      - Label:
          default: "Crawler Configuration"
        Parameters:
          - DiscoverySchedule
          - DefaultCrawlerSchedule
          - EnableAutoCrawling

Resources:
  # ============================================================================
  # S3 and Athena Resources
  # ============================================================================
  
  AthenaResultsBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      VersioningConfiguration:
        Status: Enabled
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  AthenaResultsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: ForceHTTPS
            Effect: Deny
            Principal: "*"
            Action: "s3:*"
            Resource:
              - !Sub ${AthenaResultsBucket.Arn}/*
              - !Sub ${AthenaResultsBucket.Arn}
            Condition:
              Bool:
                "aws:SecureTransport": false
      Bucket: !Ref AthenaResultsBucket

  AthenaWorkgroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: AWSPublicBlockchain
      Description: AWS Public Blockchain Data
      RecursiveDeleteOption: true
      State: ENABLED
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: true
        RequesterPaysEnabled: true
        ResultConfiguration:
          OutputLocation: !Sub s3://${AthenaResultsBucket}/results/
          EncryptionConfiguration:
            EncryptionOption: SSE_S3

  # ============================================================================
  # IAM Roles for Glue Crawlers
  # ============================================================================
  
  GlueCrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-GlueCrawlerRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub ${AthenaResultsBucket.Arn}/*
        - PolicyName: GlueCatalogPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:*Database*
                  - glue:*Table*
                  - glue:*Partition*
                Resource:
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/*
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/*/*

  # ============================================================================
  # Dynamic Database Discovery - Creates Separate DB Per Blockchain
  # ============================================================================
  
  # Lambda function that discovers new blockchains and creates databases + crawlers
  BlockchainDiscoveryFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-BlockchainDiscovery
      Description: Discovers new blockchains in S3 and creates dedicated databases and crawlers
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt BlockchainDiscoveryRole.Arn
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket
          SCHEMA_VERSION: !Ref SchemaVersion
          SCHEMA_VERSION_TON: !Ref SchemaVersionTON
          CRAWLER_ROLE_ARN: !GetAtt GlueCrawlerRole.Arn
          STACK_NAME: !Ref AWS::StackName
          SNS_TOPIC_ARN: !Ref CrawlerNotificationTopic
          DEFAULT_CRAWLER_SCHEDULE: !Ref DefaultCrawlerSchedule
      Code:
        ZipFile: |
          import boto3, json, os
          glue, s3, sns = boto3.client('glue'), boto3.client('s3'), boto3.client('sns')
          SCHEDULE_MAP = {'hourly': 'cron(0 * * * ? *)', 'daily': 'cron(0 0 * * ? *)', 'weekly': 'cron(0 0 ? * SUN *)', 'disabled': None}
          MANIFEST_FILE = 'manifest.json'
          
          def discover_from_manifest(bucket):
              """Fetch manifest.json for chain discovery. Returns {name: {path, description}} or None."""
              try:
                  resp = s3.get_object(Bucket=bucket, Key=MANIFEST_FILE, RequestPayer='requester')
                  manifest = json.loads(resp['Body'].read().decode('utf-8'))
                  if 'chains' not in manifest: return None
                  discovered = {}
                  for chain in manifest['chains']:
                      if 'name' not in chain or 'path' not in chain: continue
                      name = chain['name'].lower()
                      discovered[name] = {'path': f"s3://{bucket}/{chain['path']}", 'description': chain.get('description', f"Blockchain data for {name}")}
                      print(f"Manifest: {name} at {discovered[name]['path']}")
                  return discovered if discovered else None
              except Exception as e:
                  print(f"Manifest error: {e}")
                  return None
          
          TABLE_INDICATORS = {'blocks','transactions','logs','traces','token_transfers','contracts','receipts','events','transfers','balances','accounts','messages','operations','ledgers','payments'}
          NETWORK_INDICATORS = {'pubnet','testnet','mainnet','devnet'}
          VERSION_FOLDERS = {'v1','v2','v3','v4','v5'}
          
          def has_hive_partitions(bucket, prefix):
              try:
                  resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', MaxKeys=10, RequestPayer='requester')
                  return any('=' in p['Prefix'].rstrip('/').split('/')[-1] for p in resp.get('CommonPrefixes', []))
              except: return False
          
          def is_blockchain_folder(bucket, prefix):
              try:
                  resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', MaxKeys=20, RequestPayer='requester')
                  if any(o['Key'].endswith('.parquet') for o in resp.get('Contents', [])): return True
                  subfolders = [p['Prefix'].rstrip('/').split('/')[-1].lower() for p in resp.get('CommonPrefixes', [])]
                  if 'parquet' in subfolders: return False
                  return any(f in TABLE_INDICATORS or f in VERSION_FOLDERS or f.startswith('date=') for f in subfolders)
              except: return False
          
          def scan_for_blockchains(bucket, prefix, base_name, discovered, paginator, depth=0):
              if depth > 5: return
              try:
                  all_prefixes, parquet_prefix = [], None
                  for page in paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter='/', RequestPayer='requester'):
                      for p in page.get('CommonPrefixes', []):
                          folder = p['Prefix'].rstrip('/').split('/')[-1].lower()
                          all_prefixes.append((p['Prefix'], folder))
                          if folder == 'parquet': parquet_prefix = p['Prefix']
                  if parquet_prefix:
                      scan_for_blockchains(bucket, parquet_prefix, base_name, discovered, paginator, depth+1)
                      return
                  for sub_prefix, folder in all_prefixes:
                      if folder in VERSION_FOLDERS:
                          if has_hive_partitions(bucket, sub_prefix):
                              if base_name not in discovered: discovered[base_name] = f"s3://{bucket}/{sub_prefix}"
                          else: scan_for_blockchains(bucket, sub_prefix, base_name, discovered, paginator, depth+1)
                      elif '=' in folder: continue
                      elif is_blockchain_folder(bucket, sub_prefix):
                          if folder in NETWORK_INDICATORS:
                              scan_for_blockchains(bucket, sub_prefix, f"{base_name}_{folder}", discovered, paginator, depth+1)
                          elif folder in TABLE_INDICATORS:
                              if base_name not in discovered: discovered[base_name] = f"s3://{bucket}/{prefix}"
                              return
                          else:
                              name = f"{base_name}_{folder}"
                              if name not in discovered: discovered[name] = f"s3://{bucket}/{sub_prefix}"
                      elif folder in {'data','raw','processed'} or folder in NETWORK_INDICATORS:
                          scan_for_blockchains(bucket, sub_prefix, f"{base_name}_{folder}" if folder in NETWORK_INDICATORS else base_name, discovered, paginator, depth+1)
              except Exception as e: print(f"Error scanning {prefix}: {e}")
          
          def discover_blockchains(bucket, version):
              discovered, paginator = {}, s3.get_paginator('list_objects_v2')
              try:
                  for page in paginator.paginate(Bucket=bucket, Prefix=f"{version}/", Delimiter='/', RequestPayer='requester'):
                      for p in page.get('CommonPrefixes', []):
                          prefix, parts = p['Prefix'], p['Prefix'].strip('/').split('/')
                          if len(parts) < 2: continue
                          name = parts[1].lower()
                          if is_blockchain_folder(bucket, prefix): discovered[name] = f"s3://{bucket}/{prefix}"
                          else: scan_for_blockchains(bucket, prefix, name, discovered, paginator, depth=0)
              except Exception as e: print(f"Error scanning {version}: {e}")
              return discovered
          
          def handler(event, context):
              bucket, stack_name = os.environ['S3_BUCKET'], os.environ['STACK_NAME']
              crawler_role, sns_topic = os.environ['CRAWLER_ROLE_ARN'], os.environ['SNS_TOPIC_ARN']
              default_schedule = os.environ.get('DEFAULT_CRAWLER_SCHEDULE', 'daily')
              
              manifest_data = discover_from_manifest(bucket)
              if manifest_data:
                  all_blockchains = {n: i['path'] for n, i in manifest_data.items()}
                  blockchain_descriptions = {n: i['description'] for n, i in manifest_data.items()}
                  discovery_method = 'manifest'
              else:
                  all_blockchains, blockchain_descriptions = {}, {}
                  for ver in [os.environ['SCHEMA_VERSION'], os.environ.get('SCHEMA_VERSION_TON', 'v1.1')]:
                      for name, path in discover_blockchains(bucket, ver).items():
                          if name not in all_blockchains: all_blockchains[name] = path
                  discovery_method = 'heuristic'
              
              print(f"Discovered ({discovery_method}): {list(all_blockchains.keys())}")
              created_dbs, created_crawlers, created_schedules, started = [], [], [], []
              
              for blockchain, s3_path in all_blockchains.items():
                  db_name, crawler_name = blockchain.lower(), f"{stack_name}-{blockchain.upper()}-Crawler"
                  db_desc = f'[{stack_name}] {blockchain_descriptions.get(blockchain, f"Auto-discovered {blockchain.upper()} blockchain data")}'
                  
                  try: glue.get_database(Name=db_name)
                  except glue.exceptions.EntityNotFoundException:
                      glue.create_database(DatabaseInput={'Name': db_name, 'Description': db_desc})
                      created_dbs.append(db_name)
                  
                  schedule_expr, crawler_created = SCHEDULE_MAP.get(default_schedule), False
                  try: glue.get_crawler(Name=crawler_name)
                  except glue.exceptions.EntityNotFoundException:
                      path_parts = [p for p in s3_path.replace('s3://', '').split('/') if p]
                      table_level = len(path_parts)
                      cfg = {"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"},"Tables":{"AddOrUpdateBehavior":"MergeNewColumns"}}}
                      if table_level > 4: cfg["Grouping"] = {"TableGroupingPolicy":"CombineCompatibleSchemas","TableLevelConfiguration":table_level}
                      params = {'Name':crawler_name,'Role':crawler_role,'DatabaseName':db_name,'Description':f'Crawler for {blockchain}',
                          'Targets':{'S3Targets':[{'Path':s3_path,'SampleSize':10,'Exclusions':['**/*.xdr','**/*.xdr.zstd','**/*.json','**/*.csv','**/*.txt','**/_SUCCESS','**/_metadata','**/_common_metadata']}]},
                          'SchemaChangePolicy':{'UpdateBehavior':'LOG','DeleteBehavior':'LOG'},'RecrawlPolicy':{'RecrawlBehavior':'CRAWL_NEW_FOLDERS_ONLY'},'Configuration':json.dumps(cfg)}
                      if schedule_expr: params['Schedule'] = schedule_expr; created_schedules.append(crawler_name)
                      glue.create_crawler(**params); created_crawlers.append(crawler_name); crawler_created = True
                  
                  if crawler_created:
                      try:
                          if glue.get_crawler(Name=crawler_name)['Crawler']['State'] == 'READY':
                              glue.start_crawler(Name=crawler_name); started.append(crawler_name)
                      except: pass
              
              if created_dbs or created_crawlers:
                  msg = f"Discovery: {discovery_method}\nChains: {len(all_blockchains)}\nNew DBs: {created_dbs}\nNew Crawlers: {created_crawlers}\nStarted: {started}"
                  sns.publish(TopicArn=sns_topic, Subject=f"Blockchain Discovery: {len(all_blockchains)} chains", Message=msg)
              
              return {'statusCode':200,'body':json.dumps({'discovery_method':discovery_method,'discovered':list(all_blockchains.keys()),'created_databases':created_dbs,'created_crawlers':created_crawlers,'started_crawlers':started})}

  BlockchainDiscoveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: BlockchainDiscoveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:CreateDatabase
                  - glue:GetCrawler
                  - glue:CreateCrawler
                  - glue:StartCrawler
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt GlueCrawlerRole.Arn
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref CrawlerNotificationTopic

  # Schedule for blockchain discovery
  BlockchainDiscoverySchedule:
    Type: AWS::Events::Rule
    Condition: EnableCrawling
    Properties:
      Name: !Sub ${AWS::StackName}-BlockchainDiscoverySchedule
      Description: Schedule for discovering new blockchains
      ScheduleExpression: !Ref DiscoverySchedule
      State: ENABLED
      Targets:
        - Arn: !GetAtt BlockchainDiscoveryFunction.Arn
          Id: BlockchainDiscoveryTarget

  BlockchainDiscoveryPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref BlockchainDiscoveryFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt BlockchainDiscoverySchedule.Arn

  # ============================================================================
  # Custom Resource to Trigger Initial Discovery on Deployment
  # ============================================================================
  
  InitialDiscoveryTrigger:
    Type: Custom::InitialDiscovery
    DependsOn:
      - BlockchainDiscoveryFunction
      - GlueCrawlerRole
      - CrawlerNotificationTopic
    Properties:
      ServiceToken: !GetAtt InitialDiscoveryFunction.Arn
      DiscoveryFunctionName: !Ref BlockchainDiscoveryFunction
      StackName: !Ref AWS::StackName

  InitialDiscoveryFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-InitialDiscoveryTrigger
      Description: Triggers initial blockchain discovery on stack creation and cleanup on deletion
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt InitialDiscoveryRole.Arn
      Timeout: 900
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          
          lambda_client = boto3.client('lambda')
          glue = boto3.client('glue')
          
          def cleanup_glue_resources(stack_name):
              """Delete all Glue crawlers and databases created by this stack."""
              deleted_crawlers = []
              deleted_databases = []
              errors = []
              
              # Delete crawlers created by this stack
              try:
                  crawlers = glue.list_crawlers()['CrawlerNames']
                  for crawler_name in crawlers:
                      if crawler_name.startswith(f"{stack_name}-"):
                          try:
                              # Stop crawler if running
                              try:
                                  crawler_state = glue.get_crawler(Name=crawler_name)['Crawler']['State']
                                  if crawler_state == 'RUNNING':
                                      print(f"Stopping crawler: {crawler_name}")
                                      glue.stop_crawler(Name=crawler_name)
                                      # Wait briefly for crawler to stop
                                      import time
                                      time.sleep(5)
                              except Exception as e:
                                  print(f"Error stopping crawler {crawler_name}: {e}")
                              
                              print(f"Deleting crawler: {crawler_name}")
                              glue.delete_crawler(Name=crawler_name)
                              deleted_crawlers.append(crawler_name)
                          except Exception as e:
                              errors.append(f"Error deleting crawler {crawler_name}: {e}")
                              print(errors[-1])
              except Exception as e:
                  errors.append(f"Error listing crawlers: {e}")
                  print(errors[-1])
              
              # Delete databases (and their tables) created by THIS stack only
              # Databases are tagged with [stack_name] in their description
              try:
                  databases = glue.get_databases()['DatabaseList']
                  for db in databases:
                      db_name = db['Name']
                      description = db.get('Description', '')
                      # Only delete if description contains our stack name tag
                      if f'[{stack_name}]' in description:
                          try:
                              # Delete all tables in the database first
                              tables = glue.get_tables(DatabaseName=db_name).get('TableList', [])
                              for table in tables:
                                  print(f"Deleting table: {db_name}.{table['Name']}")
                                  glue.delete_table(DatabaseName=db_name, Name=table['Name'])
                              
                              print(f"Deleting database: {db_name}")
                              glue.delete_database(Name=db_name)
                              deleted_databases.append(db_name)
                          except Exception as e:
                              errors.append(f"Error deleting database {db_name}: {e}")
                              print(errors[-1])
              except Exception as e:
                  errors.append(f"Error listing databases: {e}")
                  print(errors[-1])
              
              return {
                  'deleted_crawlers': deleted_crawlers,
                  'deleted_databases': deleted_databases,
                  'errors': errors
              }
          
          def handler(event, context):
              print(f"Event: {json.dumps(event)}")
              
              request_type = event['RequestType']
              discovery_function = event['ResourceProperties']['DiscoveryFunctionName']
              stack_name = event['ResourceProperties']['StackName']
              
              try:
                  if request_type == 'Create':
                      # Invoke discovery function on stack creation
                      print(f"Invoking discovery function: {discovery_function}")
                      response = lambda_client.invoke(
                          FunctionName=discovery_function,
                          InvocationType='RequestResponse',
                          Payload=json.dumps({'trigger': 'initial_deployment'})
                      )
                      
                      payload = json.loads(response['Payload'].read())
                      print(f"Discovery response: {json.dumps(payload)}")
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, 
                                      {'Result': 'Initial discovery completed'},
                                      physicalResourceId='InitialDiscovery')
                  
                  elif request_type == 'Update':
                      # Optionally re-run discovery on stack update
                      print("Stack update - skipping discovery (run manually if needed)")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': 'Update - no action'},
                                      physicalResourceId='InitialDiscovery')
                  
                  elif request_type == 'Delete':
                      # Clean up Glue resources created by this stack
                      print(f"Stack delete - cleaning up Glue resources for stack: {stack_name}")
                      cleanup_result = cleanup_glue_resources(stack_name)
                      print(f"Cleanup result: {json.dumps(cleanup_result)}")
                      
                      # Always succeed on delete to not block stack deletion
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': f"Cleanup completed. Deleted {len(cleanup_result['deleted_crawlers'])} crawlers and {len(cleanup_result['deleted_databases'])} databases"},
                                      physicalResourceId='InitialDiscovery')
                      
              except Exception as e:
                  print(f"Error: {e}")
                  # On delete, always succeed to not block stack deletion
                  if request_type == 'Delete':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': f'Delete completed with errors: {str(e)}'},
                                      physicalResourceId='InitialDiscovery')
                  else:
                      cfnresponse.send(event, context, cfnresponse.FAILED,
                                      {'Error': str(e)},
                                      physicalResourceId='InitialDiscovery')

  InitialDiscoveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: InvokeDiscoveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !GetAtt BlockchainDiscoveryFunction.Arn
        - PolicyName: GlueCleanupPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:ListCrawlers
                  - glue:GetCrawler
                  - glue:StopCrawler
                  - glue:DeleteCrawler
                  - glue:GetDatabases
                  - glue:GetTables
                  - glue:DeleteTable
                  - glue:DeleteDatabase
                Resource: '*'

  # ============================================================================
  # Lambda for Post-Crawler Processing
  # ============================================================================
  
  CrawlerCompletionHandlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: GlueAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:GetCrawler
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetTables
                  - glue:UpdateTable
                Resource: '*'
        - PolicyName: SNSPublish
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref CrawlerNotificationTopic

  CrawlerCompletionHandler:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-CrawlerCompletionHandler
      Description: Processes crawler completion events and sends notifications
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt CrawlerCompletionHandlerRole.Arn
      Timeout: 300
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref CrawlerNotificationTopic
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          
          glue = boto3.client('glue')
          sns = boto3.client('sns')
          
          def dedupe_table_schema(database_name, table_name):
              """Remove columns that collide with partition keys (case-insensitive)."""
              try:
                  resp = glue.get_table(DatabaseName=database_name, Name=table_name)
                  table = resp['Table']
                  partition_keys = {pk['Name'].lower() for pk in table.get('PartitionKeys', [])}
                  if not partition_keys:
                      return None
                  
                  sd = table['StorageDescriptor']
                  original_cols = sd.get('Columns', [])
                  filtered_cols = [c for c in original_cols if c['Name'].lower() not in partition_keys]
                  removed = [c['Name'] for c in original_cols if c['Name'].lower() in partition_keys]
                  
                  if not removed:
                      return None
                  
                  # Build TableInput from existing table, keeping only mutable fields
                  table_input = {
                      'Name': table['Name'],
                      'StorageDescriptor': {**sd, 'Columns': filtered_cols},
                  }
                  for key in ['Description', 'Owner', 'Retention', 'PartitionKeys', 'TableType', 'Parameters', 'ViewOriginalText', 'ViewExpandedText']:
                      if key in table:
                          table_input[key] = table[key]
                  
                  glue.update_table(DatabaseName=database_name, TableInput=table_input)
                  return removed
              except Exception as e:
                  print(f"Error deduping {database_name}.{table_name}: {e}")
                  return None
          
          def handler(event, context):
              print(f"Received event: {json.dumps(event)}")
              
              crawler_name = event['detail']['crawlerName']
              state = event['detail']['state']
              
              if state != 'Succeeded':
                  print(f"Crawler {crawler_name} did not succeed. State: {state}")
                  return
              
              # Get crawler details
              crawler = glue.get_crawler(Name=crawler_name)
              database_name = crawler['Crawler']['DatabaseName']
              
              # Get all tables in the database (with pagination)
              tables = []
              paginator = glue.get_paginator('get_tables')
              for page in paginator.paginate(DatabaseName=database_name):
                  tables.extend(page.get('TableList', []))
              
              # De-dupe schemas
              schema_updates = {}
              for table in tables:
                  removed = dedupe_table_schema(database_name, table['Name'])
                  if removed:
                      schema_updates[table['Name']] = removed
              
              # Build notification message
              message_parts = [
                  f"Crawler Completion Report",
                  f"========================",
                  f"Crawler: {crawler_name}",
                  f"Database: {database_name}",
                  f"Completion Time: {datetime.now().isoformat()}",
                  f"",
                  f"Discovered Tables: {len(tables)}",
                  f""
              ]
              
              for table in tables:
                  table_name = table['Name']
                  # Get updated column count (after deduplication)
                  col_count = len(table['StorageDescriptor']['Columns'])
                  if table_name in schema_updates:
                      col_count -= len(schema_updates[table_name])
                  location = table['StorageDescriptor']['Location']
                  
                  message_parts.append(f"  • {table_name}")
                  message_parts.append(f"    - Columns: {col_count}")
                  message_parts.append(f"    - Location: {location}")
                  message_parts.append("")
              
              if schema_updates:
                  message_parts.append("Schema De-duplication:")
                  for tbl, cols in schema_updates.items():
                      message_parts.append(f"  • {tbl}: removed {cols}")
                  message_parts.append("")
              
              message = "\n".join(message_parts)
              
              # Send SNS notification
              sns.publish(
                  TopicArn=os.environ['SNS_TOPIC_ARN'],
                  Subject=f"Glue Crawler Completed: {crawler_name}",
                  Message=message
              )
              
              print(f"Notification sent for crawler: {crawler_name}")
              return {
                  'statusCode': 200,
                  'body': json.dumps('Processing complete')
              }

  # ============================================================================
  # EventBridge Rule for Crawler State Changes
  # ============================================================================
  
  CrawlerStateChangeRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub ${AWS::StackName}-CrawlerStateChange
      Description: Trigger Lambda when crawlers complete
      EventPattern:
        source:
          - aws.glue
        detail-type:
          - Glue Crawler State Change
        detail:
          state:
            - Succeeded
            - Failed
          crawlerName:
            - prefix: !Ref AWS::StackName
      State: ENABLED
      Targets:
        - Arn: !GetAtt CrawlerCompletionHandler.Arn
          Id: CrawlerCompletionHandlerTarget

  CrawlerCompletionHandlerPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref CrawlerCompletionHandler
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt CrawlerStateChangeRule.Arn

  # ============================================================================
  # SNS Topic for Notifications
  # ============================================================================
  
  CrawlerNotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub ${AWS::StackName}-CrawlerNotifications
      DisplayName: Blockchain Crawler Notifications
      KmsMasterKeyId: alias/aws/sns

  # ============================================================================
  # SSM Parameters (matching original template naming convention)
  # ============================================================================
  
  BucketNameParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-bucket
      Description: AWS Public Blockchain S3 bucket
      Type: String
      Value: !Ref S3Bucket

  SchemaVersionParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-schema-version
      Description: AWS Public Blockchain Schema Version
      Type: String
      Value: !Ref SchemaVersion

  SchemaVersionParameterTON:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-schema-version-ton
      Description: AWS Public Blockchain Schema Version for TON
      Type: String
      Value: !Ref SchemaVersionTON

Conditions:
  EnableCrawling: !Equals [!Ref EnableAutoCrawling, "true"]

Outputs:
  AthenaResultsBucket:
    Description: S3 bucket for Athena query results
    Value: !Ref AthenaResultsBucket
    Export:
      Name: !Sub ${AWS::StackName}-AthenaResultsBucket

  AthenaWorkgroup:
    Description: Athena workgroup for blockchain queries
    Value: !Ref AthenaWorkgroup
    Export:
      Name: !Sub ${AWS::StackName}-AthenaWorkgroup

  BlockchainDiscoveryFunction:
    Description: Lambda function for discovering new blockchains
    Value: !Ref BlockchainDiscoveryFunction
    Export:
      Name: !Sub ${AWS::StackName}-DiscoveryFunction

  CrawlerNotificationTopicArn:
    Description: SNS topic for crawler notifications
    Value: !Ref CrawlerNotificationTopic
    Export:
      Name: !Sub ${AWS::StackName}-NotificationTopic

  GlueCrawlerRoleArn:
    Description: IAM role ARN for Glue crawlers
    Value: !GetAtt GlueCrawlerRole.Arn
    Export:
      Name: !Sub ${AWS::StackName}-CrawlerRole
