AWSTemplateFormatVersion: '2010-09-09'
Description: >
  AWS Public Blockchain Data Consumer Architecture with Automated Schema Discovery.
  This template extends the base architecture with Glue Crawlers that automatically
  discover and catalog new blockchain data schemas.

Parameters:
  S3Bucket:
    Description: The S3 bucket with the AWS Public Blockchain Data
    Type: String
    Default: "aws-public-blockchain"
  
  SchemaVersion:
    Type: String
    Default: "v1.0"
    Description: Default schema version for BTC/ETH
  
  SchemaVersionTON:
    Type: String
    Default: "v1.1"
    Description: Schema version for TON blockchain
  
  DiscoverySchedule:
    Type: String
    Default: "cron(0 2 ? * SUN *)"
    Description: Schedule for blockchain discovery (default weekly on Sunday at 2 AM UTC)
  
  DefaultCrawlerSchedule:
    Type: String
    Default: "daily"
    AllowedValues:
      - "hourly"
      - "daily"
      - "weekly"
      - "disabled"
    Description: Default schedule for per-blockchain crawlers (can be changed per-chain via CLI)
  
  EnableAutoCrawling:
    Type: String
    Default: "true"
    AllowedValues:
      - "true"
      - "false"
    Description: Enable automatic crawler scheduling

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Data Source Configuration"
        Parameters:
          - S3Bucket
          - SchemaVersion
          - SchemaVersionTON
      - Label:
          default: "Crawler Configuration"
        Parameters:
          - DiscoverySchedule
          - DefaultCrawlerSchedule
          - EnableAutoCrawling

Resources:
  # ============================================================================
  # S3 and Athena Resources
  # ============================================================================
  
  AthenaResultsBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      VersioningConfiguration:
        Status: Enabled
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  AthenaResultsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Sid: ForceHTTPS
            Effect: Deny
            Principal: "*"
            Action: "s3:*"
            Resource:
              - !Sub ${AthenaResultsBucket.Arn}/*
              - !Sub ${AthenaResultsBucket.Arn}
            Condition:
              Bool:
                "aws:SecureTransport": false
      Bucket: !Ref AthenaResultsBucket

  AthenaWorkgroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: AWSPublicBlockchain
      Description: AWS Public Blockchain Data
      RecursiveDeleteOption: true
      State: ENABLED
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: true
        RequesterPaysEnabled: true
        ResultConfiguration:
          OutputLocation: !Sub s3://${AthenaResultsBucket}/results/
          EncryptionConfiguration:
            EncryptionOption: SSE_S3

  # ============================================================================
  # IAM Roles for Glue Crawlers
  # ============================================================================
  
  GlueCrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-GlueCrawlerRole
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub ${AthenaResultsBucket.Arn}/*
        - PolicyName: GlueCatalogPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:*Database*
                  - glue:*Table*
                  - glue:*Partition*
                Resource:
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/*
                  - !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/*/*

  # ============================================================================
  # Dynamic Database Discovery - Creates Separate DB Per Blockchain
  # ============================================================================
  
  # Lambda function that discovers new blockchains and creates databases + crawlers
  BlockchainDiscoveryFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-BlockchainDiscovery
      Description: Discovers new blockchains in S3 and creates dedicated databases and crawlers
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt BlockchainDiscoveryRole.Arn
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket
          SCHEMA_VERSION: !Ref SchemaVersion
          SCHEMA_VERSION_TON: !Ref SchemaVersionTON
          CRAWLER_ROLE_ARN: !GetAtt GlueCrawlerRole.Arn
          STACK_NAME: !Ref AWS::StackName
          SNS_TOPIC_ARN: !Ref CrawlerNotificationTopic
          DEFAULT_CRAWLER_SCHEDULE: !Ref DefaultCrawlerSchedule
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          
          glue = boto3.client('glue')
          s3 = boto3.client('s3')
          sns = boto3.client('sns')
          
          SCHEDULE_MAP = {
              'hourly': 'cron(0 * * * ? *)',
              'daily': 'cron(0 0 * * ? *)',
              'weekly': 'cron(0 0 ? * SUN *)',
              'disabled': None
          }
          
          # Common blockchain table names - if a folder contains these, it's a blockchain
          BLOCKCHAIN_TABLE_INDICATORS = {
              'blocks', 'transactions', 'logs', 'traces', 'token_transfers',
              'contracts', 'receipts', 'events', 'transfers', 'balances',
              'accounts', 'messages', 'operations', 'ledgers', 'payments'
          }
          
          # Network type indicators (e.g., Stellar has pubnet/testnet)
          NETWORK_INDICATORS = {'pubnet', 'testnet', 'mainnet', 'devnet'}
          
          # Intermediate folders to skip through (not blockchains themselves)
          # When we find 'parquet', we crawl ONLY that folder, ignoring siblings like 'ledgers'
          PARQUET_FOLDER = 'parquet'
          INTERMEDIATE_FOLDERS = {'data', 'raw', 'processed'}
          
          # Version folders that indicate the table root (contains date= partitions directly)
          VERSION_FOLDER_PATTERN = {'v1', 'v2', 'v3', 'v4', 'v5'}
          
          def has_hive_partitions(bucket, prefix):
              """
              Check if a folder contains Hive-style partitions (key=value format).
              This indicates the folder is a table root, not a table itself.
              """
              try:
                  response = s3.list_objects_v2(
                      Bucket=bucket,
                      Prefix=prefix,
                      Delimiter='/',
                      MaxKeys=10,
                      RequestPayer='requester'
                  )
                  for common_prefix in response.get('CommonPrefixes', []):
                      folder_name = common_prefix['Prefix'].rstrip('/').split('/')[-1]
                      # Hive partition format: key=value (e.g., date=2024-01-01)
                      if '=' in folder_name:
                          return True
                  return False
              except Exception as e:
                  print(f"Error checking for Hive partitions in {prefix}: {e}")
                  return False
          
          def is_blockchain_folder(bucket, prefix):
              """
              Determine if a folder contains blockchain data vs being a vendor namespace.
              A blockchain folder typically contains table subfolders (blocks, transactions, etc.)
              or parquet files directly, or version folders (v1, v2) with date partitions.
              
              IMPORTANT: If a 'parquet' folder exists alongside other folders (like 'ledgers'),
              this is NOT a simple blockchain folder - it needs deeper scanning to handle
              the parquet/network/version structure (e.g., Stellar).
              """
              try:
                  response = s3.list_objects_v2(
                      Bucket=bucket,
                      Prefix=prefix,
                      Delimiter='/',
                      MaxKeys=20,
                      RequestPayer='requester'
                  )
                  
                  # Check for parquet files directly in this folder
                  for obj in response.get('Contents', []):
                      if obj['Key'].endswith('.parquet'):
                          return True
                  
                  # First, check if 'parquet' folder exists - if so, this needs deeper scanning
                  subfolders = []
                  for common_prefix in response.get('CommonPrefixes', []):
                      folder_name = common_prefix['Prefix'].rstrip('/').split('/')[-1].lower()
                      subfolders.append(folder_name)
                      if folder_name == PARQUET_FOLDER:
                          # Has parquet folder - NOT a simple blockchain, needs scan_for_blockchains
                          return False
                  
                  # Check subfolders for blockchain table indicators or version folders
                  for folder_name in subfolders:
                      if folder_name in BLOCKCHAIN_TABLE_INDICATORS:
                          return True
                      # Check for version folders (v1, v2, etc.) - these are table roots
                      if folder_name in VERSION_FOLDER_PATTERN:
                          return True
                      # Check for date partition pattern (date=YYYY-MM-DD)
                      if folder_name.startswith('date='):
                          return True
                  
                  return False
              except Exception as e:
                  print(f"Error checking if blockchain folder {prefix}: {e}")
                  return False
          
          def scan_for_blockchains(bucket, prefix, base_name, discovered, paginator, depth=0):
              """
              Recursively scan for blockchain data, handling nested structures like:
              - v1.1/stellar/parquet/pubnet/v1/date=.../
              - v1.1/stellar/parquet/testnet/v1/date=.../
              
              When a 'parquet' folder exists, we ONLY crawl that folder and ignore
              sibling folders (like 'ledgers' which may contain raw XDR files).
              
              For Stellar-style structures where version folders (v1, v2) contain
              Hive-style date partitions directly, we set the crawler path at the 
              version folder level so Glue recognizes it as a single partitioned table.
              
              Args:
                  bucket: S3 bucket name
                  prefix: Current S3 prefix to scan
                  base_name: Base blockchain name (e.g., 'stellar')
                  discovered: Dict to add discovered blockchains to
                  paginator: S3 paginator
                  depth: Current recursion depth (max 5 to prevent infinite loops)
              """
              if depth > 5:
                  return
              
              try:
                  pages = paginator.paginate(
                      Bucket=bucket,
                      Prefix=prefix,
                      Delimiter='/',
                      RequestPayer='requester'
                  )
                  
                  # First pass: collect all prefixes and check for parquet folder
                  all_prefixes = []
                  has_parquet_folder = False
                  parquet_prefix = None
                  
                  for page in pages:
                      for prefix_obj in page.get('CommonPrefixes', []):
                          sub_prefix = prefix_obj['Prefix']
                          folder_name = sub_prefix.rstrip('/').split('/')[-1].lower()
                          all_prefixes.append((sub_prefix, folder_name))
                          if folder_name == PARQUET_FOLDER:
                              has_parquet_folder = True
                              parquet_prefix = sub_prefix
                  
                  # If parquet folder exists, ONLY process that (skip siblings like 'ledgers')
                  if has_parquet_folder:
                      print(f"  Found parquet folder, scanning only: {parquet_prefix}")
                      scan_for_blockchains(bucket, parquet_prefix, base_name, discovered, paginator, depth + 1)
                      return
                  
                  # Process all subfolders
                  for sub_prefix, folder_name in all_prefixes:
                      
                      # Check for version folders (v1, v2) - these are table roots for Stellar-style data
                      if folder_name in VERSION_FOLDER_PATTERN:
                          # Verify this version folder has Hive partitions (date=...)
                          if has_hive_partitions(bucket, sub_prefix):
                              # This version folder IS the table root - set crawler path here
                              s3_path = f"s3://{bucket}/{sub_prefix}"
                              if base_name not in discovered:
                                  discovered[base_name] = s3_path
                                  print(f"Found blockchain (version folder with partitions): {base_name} at {s3_path}")
                          else:
                              # Version folder without Hive partitions - recurse deeper
                              scan_for_blockchains(bucket, sub_prefix, base_name, discovered, paginator, depth + 1)
                          continue
                      
                      # Check for Hive-style partitions directly (date=YYYY-MM-DD)
                      # If we see these, the PARENT folder is the table root
                      if '=' in folder_name:
                          # Parent is the table root, already handled by caller
                          continue
                      
                      # Check if this folder contains blockchain tables
                      if is_blockchain_folder(bucket, sub_prefix):
                          # Determine the database name
                          if folder_name in NETWORK_INDICATORS:
                              # Network-specific: stellar_pubnet, stellar_testnet
                              db_name = f"{base_name}_{folder_name}"
                              # Recurse into network folder to find version folders
                              print(f"  Found network folder: {folder_name}, scanning for version folders...")
                              scan_for_blockchains(bucket, sub_prefix, db_name, discovered, paginator, depth + 1)
                          elif folder_name in BLOCKCHAIN_TABLE_INDICATORS:
                              # This is a table folder (blocks, transactions, etc.)
                              # The PARENT folder is the blockchain root - set crawler there
                              s3_path = f"s3://{bucket}/{prefix}"
                              if base_name not in discovered:
                                  discovered[base_name] = s3_path
                                  print(f"Found blockchain (from table indicator): {base_name} at {s3_path}")
                              # Don't continue processing siblings - we've found the blockchain root
                              return
                          else:
                              # This is a sub-chain under a vendor namespace (e.g., sonarx/aptos)
                              # Create separate database for each sub-chain
                              sub_chain_name = f"{base_name}_{folder_name}"
                              s3_path = f"s3://{bucket}/{sub_prefix}"
                              if sub_chain_name not in discovered:
                                  discovered[sub_chain_name] = s3_path
                                  print(f"Found blockchain (vendor sub-chain): {sub_chain_name} at {s3_path}")
                          continue
                      
                      # If it's an intermediate folder, recurse into it
                      if folder_name in INTERMEDIATE_FOLDERS:
                          print(f"  Scanning intermediate folder: {folder_name}")
                          scan_for_blockchains(bucket, sub_prefix, base_name, discovered, paginator, depth + 1)
                          continue
                      
                      # If it's a network indicator folder, recurse with network-aware naming
                      if folder_name in NETWORK_INDICATORS:
                          print(f"  Found network folder: {folder_name}")
                          network_name = f"{base_name}_{folder_name}"
                          scan_for_blockchains(bucket, sub_prefix, network_name, discovered, paginator, depth + 1)
                          
              except Exception as e:
                  print(f"Error scanning {prefix}: {e}")
          
          def discover_blockchains(bucket, version):
              """
              Discover blockchain namespaces, automatically detecting vendor directories
              and nested structures like stellar/parquet/pubnet/.
              Returns dict of blockchain_name -> s3_path
              """
              discovered = {}
              paginator = s3.get_paginator('list_objects_v2')
              
              try:
                  pages = paginator.paginate(
                      Bucket=bucket,
                      Prefix=f"{version}/",
                      Delimiter='/',
                      RequestPayer='requester'
                  )
                  for page in pages:
                      for prefix_obj in page.get('CommonPrefixes', []):
                          prefix = prefix_obj['Prefix']
                          parts = prefix.strip('/').split('/')
                          if len(parts) < 2:
                              continue
                          
                          name = parts[1].lower()
                          
                          # Check if this is a blockchain folder directly
                          if is_blockchain_folder(bucket, prefix):
                              s3_path = f"s3://{bucket}/{prefix}"
                              discovered[name] = s3_path
                              print(f"Found blockchain: {name} at {s3_path}")
                          else:
                              # Scan deeper for nested structures
                              print(f"Scanning {name} for nested blockchain data...")
                              scan_for_blockchains(bucket, prefix, name, discovered, paginator, depth=0)
                              
              except Exception as e:
                  print(f"Error scanning {version}: {e}")
              
              return discovered
          
          def handler(event, context):
              print(f"Event: {json.dumps(event)}")
              
              bucket = os.environ['S3_BUCKET']
              schema_version = os.environ['SCHEMA_VERSION']
              schema_version_ton = os.environ.get('SCHEMA_VERSION_TON', 'v1.1')
              crawler_role = os.environ['CRAWLER_ROLE_ARN']
              stack_name = os.environ['STACK_NAME']
              sns_topic = os.environ['SNS_TOPIC_ARN']
              default_schedule = os.environ.get('DEFAULT_CRAWLER_SCHEDULE', 'daily')
              
              # Discover blockchain namespaces in S3 (auto-detects vendor namespaces)
              all_blockchains = {}  # blockchain_name -> s3_path
              for version in [schema_version, schema_version_ton]:
                  discovered = discover_blockchains(bucket, version)
                  # Only add if not already discovered (prefer earlier schema version)
                  for name, path in discovered.items():
                      if name not in all_blockchains:
                          all_blockchains[name] = path
              
              print(f"Discovered blockchains: {list(all_blockchains.keys())}")
              
              created_databases = []
              created_crawlers = []
              created_schedules = []
              started_crawlers = []
              
              for blockchain, s3_path in all_blockchains.items():
                  db_name = blockchain.lower()
                  crawler_name = f"{stack_name}-{blockchain.upper()}-Crawler"
                  
                  # Create database if it doesn't exist
                  try:
                      glue.get_database(Name=db_name)
                      print(f"Database {db_name} already exists")
                  except glue.exceptions.EntityNotFoundException:
                      print(f"Creating database: {db_name}")
                      glue.create_database(
                          DatabaseInput={
                              'Name': db_name,
                              'Description': f'[{stack_name}] Auto-discovered {blockchain.upper()} blockchain data'
                          }
                      )
                      created_databases.append(db_name)
                  
                  # s3_path is already determined from discovery
                  print(f"Using S3 path for {blockchain}: {s3_path}")
                  
                  # Create crawler if it doesn't exist (with built-in schedule)
                  crawler_created = False
                  schedule_expr = SCHEDULE_MAP.get(default_schedule)
                  try:
                      glue.get_crawler(Name=crawler_name)
                      print(f"Crawler {crawler_name} already exists")
                  except glue.exceptions.EntityNotFoundException:
                      print(f"Creating crawler: {crawler_name} for {s3_path}")
                      # Calculate table level from S3 path depth
                      # s3://bucket/a/b/c/ = 3 levels after bucket
                      path_parts = s3_path.replace('s3://', '').split('/')
                      # Filter empty parts and count (bucket + path segments)
                      table_level = len([p for p in path_parts if p])
                      
                      # Determine if this is a deep path (like Stellar's parquet/pubnet/v1/)
                      # Deep paths need TableGroupingPolicy to combine partitions into one table
                      # Standard paths (btc/, eth/) should let Glue create separate tables naturally
                      is_deep_path = table_level > 4  # More than bucket/version/chain/table
                      
                      crawler_config = {
                          "Version": 1.0,
                          "CrawlerOutput": {
                              "Partitions": {"AddOrUpdateBehavior": "InheritFromTable"},
                              "Tables": {"AddOrUpdateBehavior": "MergeNewColumns"}
                          }
                      }
                      
                      # Only add grouping policy for deep paths (Stellar-style)
                      if is_deep_path:
                          crawler_config["Grouping"] = {
                              "TableGroupingPolicy": "CombineCompatibleSchemas",
                              "TableLevelConfiguration": table_level
                          }
                      
                      crawler_params = {
                          'Name': crawler_name,
                          'Role': crawler_role,
                          'DatabaseName': db_name,
                          'Description': f'Auto-created crawler for {blockchain.upper()} blockchain',
                          'Targets': {
                              'S3Targets': [{
                                  'Path': s3_path,
                                  'SampleSize': 10,
                                  'Exclusions': [
                                      '**/*.xdr',
                                      '**/*.xdr.zstd',
                                      '**/*.json',
                                      '**/*.csv',
                                      '**/*.txt',
                                      '**/_SUCCESS',
                                      '**/_metadata',
                                      '**/_common_metadata'
                                  ]
                              }]
                          },
                          'SchemaChangePolicy': {
                              'UpdateBehavior': 'LOG',
                              'DeleteBehavior': 'LOG'
                          },
                          'RecrawlPolicy': {
                              'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'
                          },
                          'Configuration': json.dumps(crawler_config)
                      }
                      # Add schedule if not disabled
                      if schedule_expr:
                          crawler_params['Schedule'] = schedule_expr
                          created_schedules.append(f"{crawler_name} ({default_schedule})")
                      
                      glue.create_crawler(**crawler_params)
                      created_crawlers.append(crawler_name)
                      crawler_created = True
                  
                  # Start the crawler on first creation
                  if crawler_created:
                      try:
                          crawler_state = glue.get_crawler(Name=crawler_name)['Crawler']['State']
                          if crawler_state == 'READY':
                              print(f"Starting crawler: {crawler_name}")
                              glue.start_crawler(Name=crawler_name)
                              started_crawlers.append(crawler_name)
                          else:
                              print(f"Crawler {crawler_name} is {crawler_state}, skipping start")
                      except Exception as e:
                          print(f"Error starting crawler {crawler_name}: {e}")
              
              # Send notification
              if created_databases or created_crawlers or created_schedules:
                  message = f"""
          Blockchain Discovery Report
          ============================
          
          Blockchains Found: {len(all_blockchains)}
          {chr(10).join(f'  - {b}' for b in sorted(all_blockchains.keys()))}
          
          New Databases Created: {len(created_databases)}
          {chr(10).join(f'  - {db}' for db in created_databases) if created_databases else '  None'}
          
          New Crawlers Created: {len(created_crawlers)}
          {chr(10).join(f'  - {c}' for c in created_crawlers) if created_crawlers else '  None'}
          
          New Schedules Created: {len(created_schedules)}
          {chr(10).join(f'  - {s}' for s in created_schedules) if created_schedules else '  None'}
          
          Crawlers Started: {len(started_crawlers)}
          {chr(10).join(f'  - {c}' for c in started_crawlers) if started_crawlers else '  None'}
          
          Tables will be available in Athena once crawlers complete.
          
          To change a crawler's schedule, use the AWS Console or CLI:
            aws glue update-crawler --name <crawler-name> --schedule "cron(0 * * * ? *)"
          """
                  
                  sns.publish(
                      TopicArn=sns_topic,
                      Subject=f"Blockchain Discovery: {len(all_blockchains)} chains found",
                      Message=message
                  )
              
              return {
                  'statusCode': 200,
                  'body': json.dumps({
                      'discovered': list(all_blockchains.keys()),
                      'created_databases': created_databases,
                      'created_crawlers': created_crawlers,
                      'created_schedules': created_schedules,
                      'started_crawlers': started_crawlers
                  })
              }

  BlockchainDiscoveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: BlockchainDiscoveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:CreateDatabase
                  - glue:GetCrawler
                  - glue:CreateCrawler
                  - glue:StartCrawler
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt GlueCrawlerRole.Arn
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref CrawlerNotificationTopic

  # Schedule for blockchain discovery
  BlockchainDiscoverySchedule:
    Type: AWS::Events::Rule
    Condition: EnableCrawling
    Properties:
      Name: !Sub ${AWS::StackName}-BlockchainDiscoverySchedule
      Description: Schedule for discovering new blockchains
      ScheduleExpression: !Ref DiscoverySchedule
      State: ENABLED
      Targets:
        - Arn: !GetAtt BlockchainDiscoveryFunction.Arn
          Id: BlockchainDiscoveryTarget

  BlockchainDiscoveryPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref BlockchainDiscoveryFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt BlockchainDiscoverySchedule.Arn

  # ============================================================================
  # Custom Resource to Trigger Initial Discovery on Deployment
  # ============================================================================
  
  InitialDiscoveryTrigger:
    Type: Custom::InitialDiscovery
    DependsOn:
      - BlockchainDiscoveryFunction
      - GlueCrawlerRole
      - CrawlerNotificationTopic
    Properties:
      ServiceToken: !GetAtt InitialDiscoveryFunction.Arn
      DiscoveryFunctionName: !Ref BlockchainDiscoveryFunction
      StackName: !Ref AWS::StackName

  InitialDiscoveryFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-InitialDiscoveryTrigger
      Description: Triggers initial blockchain discovery on stack creation and cleanup on deletion
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt InitialDiscoveryRole.Arn
      Timeout: 900
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          
          lambda_client = boto3.client('lambda')
          glue = boto3.client('glue')
          
          def cleanup_glue_resources(stack_name):
              """Delete all Glue crawlers and databases created by this stack."""
              deleted_crawlers = []
              deleted_databases = []
              errors = []
              
              # Delete crawlers created by this stack
              try:
                  crawlers = glue.list_crawlers()['CrawlerNames']
                  for crawler_name in crawlers:
                      if crawler_name.startswith(f"{stack_name}-"):
                          try:
                              # Stop crawler if running
                              try:
                                  crawler_state = glue.get_crawler(Name=crawler_name)['Crawler']['State']
                                  if crawler_state == 'RUNNING':
                                      print(f"Stopping crawler: {crawler_name}")
                                      glue.stop_crawler(Name=crawler_name)
                                      # Wait briefly for crawler to stop
                                      import time
                                      time.sleep(5)
                              except Exception as e:
                                  print(f"Error stopping crawler {crawler_name}: {e}")
                              
                              print(f"Deleting crawler: {crawler_name}")
                              glue.delete_crawler(Name=crawler_name)
                              deleted_crawlers.append(crawler_name)
                          except Exception as e:
                              errors.append(f"Error deleting crawler {crawler_name}: {e}")
                              print(errors[-1])
              except Exception as e:
                  errors.append(f"Error listing crawlers: {e}")
                  print(errors[-1])
              
              # Delete databases (and their tables) created by THIS stack only
              # Databases are tagged with [stack_name] in their description
              try:
                  databases = glue.get_databases()['DatabaseList']
                  for db in databases:
                      db_name = db['Name']
                      description = db.get('Description', '')
                      # Only delete if description contains our stack name tag
                      if f'[{stack_name}]' in description:
                          try:
                              # Delete all tables in the database first
                              tables = glue.get_tables(DatabaseName=db_name).get('TableList', [])
                              for table in tables:
                                  print(f"Deleting table: {db_name}.{table['Name']}")
                                  glue.delete_table(DatabaseName=db_name, Name=table['Name'])
                              
                              print(f"Deleting database: {db_name}")
                              glue.delete_database(Name=db_name)
                              deleted_databases.append(db_name)
                          except Exception as e:
                              errors.append(f"Error deleting database {db_name}: {e}")
                              print(errors[-1])
              except Exception as e:
                  errors.append(f"Error listing databases: {e}")
                  print(errors[-1])
              
              return {
                  'deleted_crawlers': deleted_crawlers,
                  'deleted_databases': deleted_databases,
                  'errors': errors
              }
          
          def handler(event, context):
              print(f"Event: {json.dumps(event)}")
              
              request_type = event['RequestType']
              discovery_function = event['ResourceProperties']['DiscoveryFunctionName']
              stack_name = event['ResourceProperties']['StackName']
              
              try:
                  if request_type == 'Create':
                      # Invoke discovery function on stack creation
                      print(f"Invoking discovery function: {discovery_function}")
                      response = lambda_client.invoke(
                          FunctionName=discovery_function,
                          InvocationType='RequestResponse',
                          Payload=json.dumps({'trigger': 'initial_deployment'})
                      )
                      
                      payload = json.loads(response['Payload'].read())
                      print(f"Discovery response: {json.dumps(payload)}")
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, 
                                      {'Result': 'Initial discovery completed'},
                                      physicalResourceId='InitialDiscovery')
                  
                  elif request_type == 'Update':
                      # Optionally re-run discovery on stack update
                      print("Stack update - skipping discovery (run manually if needed)")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': 'Update - no action'},
                                      physicalResourceId='InitialDiscovery')
                  
                  elif request_type == 'Delete':
                      # Clean up Glue resources created by this stack
                      print(f"Stack delete - cleaning up Glue resources for stack: {stack_name}")
                      cleanup_result = cleanup_glue_resources(stack_name)
                      print(f"Cleanup result: {json.dumps(cleanup_result)}")
                      
                      # Always succeed on delete to not block stack deletion
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': f"Cleanup completed. Deleted {len(cleanup_result['deleted_crawlers'])} crawlers and {len(cleanup_result['deleted_databases'])} databases"},
                                      physicalResourceId='InitialDiscovery')
                      
              except Exception as e:
                  print(f"Error: {e}")
                  # On delete, always succeed to not block stack deletion
                  if request_type == 'Delete':
                      cfnresponse.send(event, context, cfnresponse.SUCCESS,
                                      {'Result': f'Delete completed with errors: {str(e)}'},
                                      physicalResourceId='InitialDiscovery')
                  else:
                      cfnresponse.send(event, context, cfnresponse.FAILED,
                                      {'Error': str(e)},
                                      physicalResourceId='InitialDiscovery')

  InitialDiscoveryRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: InvokeDiscoveryPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !GetAtt BlockchainDiscoveryFunction.Arn
        - PolicyName: GlueCleanupPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:ListCrawlers
                  - glue:GetCrawler
                  - glue:StopCrawler
                  - glue:DeleteCrawler
                  - glue:GetDatabases
                  - glue:GetTables
                  - glue:DeleteTable
                  - glue:DeleteDatabase
                Resource: '*'

  # ============================================================================
  # Lambda for Post-Crawler Processing
  # ============================================================================
  
  CrawlerCompletionHandlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: GlueAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:GetCrawler
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetTables
                  - glue:UpdateTable
                Resource: '*'
        - PolicyName: SNSPublish
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref CrawlerNotificationTopic

  CrawlerCompletionHandler:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}-CrawlerCompletionHandler
      Description: Processes crawler completion events and sends notifications
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt CrawlerCompletionHandlerRole.Arn
      Timeout: 300
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref CrawlerNotificationTopic
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          
          glue = boto3.client('glue')
          sns = boto3.client('sns')
          
          def dedupe_table_schema(database_name, table_name):
              """Remove columns that collide with partition keys (case-insensitive)."""
              try:
                  resp = glue.get_table(DatabaseName=database_name, Name=table_name)
                  table = resp['Table']
                  partition_keys = {pk['Name'].lower() for pk in table.get('PartitionKeys', [])}
                  if not partition_keys:
                      return None
                  
                  sd = table['StorageDescriptor']
                  original_cols = sd.get('Columns', [])
                  filtered_cols = [c for c in original_cols if c['Name'].lower() not in partition_keys]
                  removed = [c['Name'] for c in original_cols if c['Name'].lower() in partition_keys]
                  
                  if not removed:
                      return None
                  
                  # Build TableInput from existing table, keeping only mutable fields
                  table_input = {
                      'Name': table['Name'],
                      'StorageDescriptor': {**sd, 'Columns': filtered_cols},
                  }
                  for key in ['Description', 'Owner', 'Retention', 'PartitionKeys', 'TableType', 'Parameters', 'ViewOriginalText', 'ViewExpandedText']:
                      if key in table:
                          table_input[key] = table[key]
                  
                  glue.update_table(DatabaseName=database_name, TableInput=table_input)
                  return removed
              except Exception as e:
                  print(f"Error deduping {database_name}.{table_name}: {e}")
                  return None
          
          def handler(event, context):
              print(f"Received event: {json.dumps(event)}")
              
              crawler_name = event['detail']['crawlerName']
              state = event['detail']['state']
              
              if state != 'Succeeded':
                  print(f"Crawler {crawler_name} did not succeed. State: {state}")
                  return
              
              # Get crawler details
              crawler = glue.get_crawler(Name=crawler_name)
              database_name = crawler['Crawler']['DatabaseName']
              
              # Get all tables in the database (with pagination)
              tables = []
              paginator = glue.get_paginator('get_tables')
              for page in paginator.paginate(DatabaseName=database_name):
                  tables.extend(page.get('TableList', []))
              
              # De-dupe schemas
              schema_updates = {}
              for table in tables:
                  removed = dedupe_table_schema(database_name, table['Name'])
                  if removed:
                      schema_updates[table['Name']] = removed
              
              # Build notification message
              message_parts = [
                  f"Crawler Completion Report",
                  f"========================",
                  f"Crawler: {crawler_name}",
                  f"Database: {database_name}",
                  f"Completion Time: {datetime.now().isoformat()}",
                  f"",
                  f"Discovered Tables: {len(tables)}",
                  f""
              ]
              
              for table in tables:
                  table_name = table['Name']
                  # Get updated column count (after deduplication)
                  col_count = len(table['StorageDescriptor']['Columns'])
                  if table_name in schema_updates:
                      col_count -= len(schema_updates[table_name])
                  location = table['StorageDescriptor']['Location']
                  
                  message_parts.append(f"  • {table_name}")
                  message_parts.append(f"    - Columns: {col_count}")
                  message_parts.append(f"    - Location: {location}")
                  message_parts.append("")
              
              if schema_updates:
                  message_parts.append("Schema De-duplication:")
                  for tbl, cols in schema_updates.items():
                      message_parts.append(f"  • {tbl}: removed {cols}")
                  message_parts.append("")
              
              message = "\n".join(message_parts)
              
              # Send SNS notification
              sns.publish(
                  TopicArn=os.environ['SNS_TOPIC_ARN'],
                  Subject=f"Glue Crawler Completed: {crawler_name}",
                  Message=message
              )
              
              print(f"Notification sent for crawler: {crawler_name}")
              return {
                  'statusCode': 200,
                  'body': json.dumps('Processing complete')
              }

  # ============================================================================
  # EventBridge Rule for Crawler State Changes
  # ============================================================================
  
  CrawlerStateChangeRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub ${AWS::StackName}-CrawlerStateChange
      Description: Trigger Lambda when crawlers complete
      EventPattern:
        source:
          - aws.glue
        detail-type:
          - Glue Crawler State Change
        detail:
          state:
            - Succeeded
            - Failed
          crawlerName:
            - prefix: !Ref AWS::StackName
      State: ENABLED
      Targets:
        - Arn: !GetAtt CrawlerCompletionHandler.Arn
          Id: CrawlerCompletionHandlerTarget

  CrawlerCompletionHandlerPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref CrawlerCompletionHandler
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt CrawlerStateChangeRule.Arn

  # ============================================================================
  # SNS Topic for Notifications
  # ============================================================================
  
  CrawlerNotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub ${AWS::StackName}-CrawlerNotifications
      DisplayName: Blockchain Crawler Notifications
      KmsMasterKeyId: alias/aws/sns

  # ============================================================================
  # SSM Parameters (matching original template naming convention)
  # ============================================================================
  
  BucketNameParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-bucket
      Description: AWS Public Blockchain S3 bucket
      Type: String
      Value: !Ref S3Bucket

  SchemaVersionParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-schema-version
      Description: AWS Public Blockchain Schema Version
      Type: String
      Value: !Ref SchemaVersion

  SchemaVersionParameterTON:
    Type: AWS::SSM::Parameter
    Properties:
      Name: public-blockchain-schema-version-ton
      Description: AWS Public Blockchain Schema Version for TON
      Type: String
      Value: !Ref SchemaVersionTON

Conditions:
  EnableCrawling: !Equals [!Ref EnableAutoCrawling, "true"]

Outputs:
  AthenaResultsBucket:
    Description: S3 bucket for Athena query results
    Value: !Ref AthenaResultsBucket
    Export:
      Name: !Sub ${AWS::StackName}-AthenaResultsBucket

  AthenaWorkgroup:
    Description: Athena workgroup for blockchain queries
    Value: !Ref AthenaWorkgroup
    Export:
      Name: !Sub ${AWS::StackName}-AthenaWorkgroup

  BlockchainDiscoveryFunction:
    Description: Lambda function for discovering new blockchains
    Value: !Ref BlockchainDiscoveryFunction
    Export:
      Name: !Sub ${AWS::StackName}-DiscoveryFunction

  CrawlerNotificationTopicArn:
    Description: SNS topic for crawler notifications
    Value: !Ref CrawlerNotificationTopic
    Export:
      Name: !Sub ${AWS::StackName}-NotificationTopic

  GlueCrawlerRoleArn:
    Description: IAM role ARN for Glue crawlers
    Value: !GetAtt GlueCrawlerRole.Arn
    Export:
      Name: !Sub ${AWS::StackName}-CrawlerRole
